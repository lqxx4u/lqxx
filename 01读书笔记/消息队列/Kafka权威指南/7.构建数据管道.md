#### 7构建数据管道

使用场景：

（1）把Kafka作为数据管道的两个端点之一

（2）把Kafka作为数据管道两个端点的中间媒介

#### 7.1 构建数据管道需要考虑的问题

#### 1. 及时性
实时的生产者和基于批处理的消费者可以同时存在，也可以任意组合。

回压策略

讨论回压的问题，有两个前提：

>发布者与订阅者不在同一个线程中，因为在同一个线程中的话，通常使用传统的逻辑就可以，不需要进行回压处理；

>发布者发出数据的速度高于订阅者处理数据的速度，也就是处于“PUSH”状态下，如果相反，那就是“PUll”状态，不需要处理回压。

回压的处理有以下几种策略：

    1.ERROR： 当下游跟不上节奏的时候发出一个错误信号。
    2.DROP：当下游没有准备好接收新的元素的时候抛弃这个元素。
    3.LATEST：让下游只得到上游最新的元素。
    4.BUFFER：缓存下游没有来得及处理的元素（如果缓存不限大小的可能导致OutOfMemoryError）。

#### 2. 可靠性
    至少一次传递
    仅一次传递


#### 3. 高吞吐量和动态吞吐量
    数据积压在kafka
    额外增加消费者或生产者以实现Kafka的伸缩
    Connect API支持伸缩、擅长并行处理任务
    Kafka支持多种类型的压缩


#### 4. 数据格式
数据管道需要协调各种数据格式和数据类型

生产者和消费者可以使用各种序列化器来表示任意格式的数据。

Connect API有自己的内存对象模型，包括数据类型和 schema。不过可以使用可插拔的转换器将这些对象保存成任意格式。

#### 5. 转换
ETL: 提取-转换-加载（Extract - Transform - Load）

ELT: 提取 - 加载 - 转换（Extract - Load - Transform）

#### 6. 安全性
支持加密传输数据

支持认证（通过SASL实现）和授权

提供审计日志用于跟踪访问记录


#### 7. 故障处理能力
	把缺损的数据挡在数据管道之外
	恢复无法解析的记录
	修复并重新处理缺损的数据
	若干天之后发现原来看起来正常的数据其实是缺损数据，重新处理
	
#### 8. 耦合性和灵活性
数据管道最重要的作用之一是解耦数据源和数据池

临时数据管道
    
新系统加入时，不需要构建额外的数据管道

元数据丢失
	
允许schema发送变更，应用程序各方修改自己的代码，不会对整个系统造成破坏

末端处理
	
尽量保留原始数据的完整性，让下游应用自己决定如何处理和聚合数据。


#### 7.2 如何在Connect API和客户端API之间作出选择
##### 使用传统的生产者和消费者客户端
##### 使用Connect API和连接器
    Connect：配置管理、偏移量存储、并行处理、错误处理、支持多种数据类型和标准的REST管理API
    连接Kafka和外部数据存储系统应用细节：数据类型和配置选项

#### 7.3 Kafka Connect
Connect以worker集群的方式运行，我们基于worker进程安装连接器插件，然后使用REST API来管理和配置connector，这些worker进程都是长时间持续运行的作业。

连接器启动额外的task，有效地利用工作节点的资源，以并行的方式移动大量的数据。

数据源的连接器负责从源系统读取数据，并把数据对象提供给worker进程。

数据池的连接器负责从worker进程获取数据，并把他们写入目标系统。
##### 7.3.1 运行Connect
所有机器安装Kafka，部分服务器上启动broker，然后在其他服务器启动Connect

启动Connect进程与启动broker差不多
>bin/connect-distributed.sh config/connect-distributed.properties &

启动work集群后通过REST API 来验证、配置和监控
>验证是否正常： curl http://localhost:8083
>
>检查已经安装好的连接器插件： curl:http://localhost:8083/connector-plugins

单机模式：
>启动：bin/connect-standalone.sh

##### 7.3.2连接器示例 -- 文件数据源和文件数据池
##### 7.3.3连接器示例 -- 从MySQL到ElasticSearch
[构建自己的连接器](http://docs.confluent.io/3.0.1/connect/devguide.html)

##### 7.3.4 深入理解Connect
##### 1. 连接器和任务
##### 连接器负责以下三件事

	决定运行多少个任务
	按照任务来拆分数据复制
	从worker进程获取任务配置并将其传递下去
##### 任务
任务负责将数据移入或移出Kafka。任务在初始化时会得到由worker进程分配的一个上下文。
>源系统上下文包含了一个对象，可以将源系统记录的偏移量保存在上下文里。
>文件连接器的偏移量就是文件里的字节位置
>JDBC连接器的偏移量可以是数据表的主键ID
>
目标系统连接器的上下文提供了一些方法，连接器可以用他们操作从Kafka接收到的数据。（数据清理、错误重试、偏移量保存到外部系统以便实现仅一次的传递）

任务在完成初始化之后，开始按照连接器指定的配置（包含在一个Properties对象里）启动工作。

源系统任务对外部系统进行轮询，并返回一些记录，worker进程将这些记录发送到Kafka。

数据池任务通过worker进程接收来自Kafka的记录，并将它们写入外部系统。



##### 2. worker进程
负责REST API 、配置管理、可靠性、高可用性、伸缩性和负载均衡、偏移量管理

连接器插件帮你处理配置、异常、REST API、监控、部署、伸缩、失效等问题
##### 3. 转化器和Connect的数据模型
Connect提供一组数据API， 它们包含了数据对象和用于描述数据的schema。
>JDBC连接器从数据库读取一个字段，并基于这个字段的数据类型创建一个 Connect Schema对象。
>然后使用这些 Schema对象创建一个包含了所有数据库字段的Struct -- 我们保存了每一个字段的名字和它们的值。

源连接器只负责基于Data API生成数据对象

worker怎么将数据对象保存到kafka的：使用转换器

连接器通过Data API将数据返回给worker进程，worker进程使用指定的转换器将数据转换成Avro对象、JSON对象或者字符串，然后将它们写入kafka。


##### 4. 偏移量管理
源连接器返回给worker进程的记录包含源系统的分区和偏移量，worker进程将这些记录发送给Kafka。如果Kafka确认记录保存成功，worker进程就把偏移量保存下来。

偏移量的存储机制是可插拔的，一般会使用Kafka主题来保存。如果连接器发送崩溃并重启，它可以从最近的偏移量继续处理数据。


#### 7.4Connect之外的选择
##### 1. 用于其他数据存储的摄入框架
Hadoop 使用 Flume

ElasticSearch使用Logstash或Fluentd
##### 2. 基于图形界面的ETL工具
将Kafka当成是一个支持数据集成（使用Connect）、应用集成（使用生产者和消费者）和流式处理的平台

##### 3.流式处理框架
