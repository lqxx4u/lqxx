发布订阅 度量指标，更长时间的度量指标

发布订阅日志

发布订阅跟踪

此时需要单一的集中式系统，发布通用类型的数据，规模可以随业务增长

Kafka分布式提交日志或者分布式流平台

Kafka按照一定顺序持久化保存。

#### 1.2.1 消息和批次
Kafka的数据单元被称为消息，消息可以有一个可选的元数据，也就是键。键也是一个字节数组，与消息一样，对于Kafka来说也没有特殊的含义。

当消息以一种可控的方式写入不同的分区时，会用到键。最简单的例子就是为键生成一个一致性散列，然后使用散列值对主题分区数进行取模，为消息选取分区。这样可以保证具有相同键的消息总是被写到相同的分区上。

为了提高效率，消息被分批次写入Kafka。批次就是一组消息，这些消息同属一个主题和分区。

#### 1.2.2 模式
以额外的结构来定义消息内容，让他们更易于理解。

根据应用程序的需求，消息模式有许多可用的选项。json ,xml 

Kafka开发者喜欢用apache avro,它最初是为hadoop开发的一款序列化框架。

#### 1.2.3主题和分区
##### Kafka的消息通过主题进行分类。
主题可以被分为若干分区，一个分区就是一个提交日志。

消息以追加的方式写入分区，然后以先入先出的顺序读取。

注意：由于一个主题一般包含几个分区，因此无法在整个主题范围内保证消息的顺序，但可以保证消息在单个分区内的顺序。

Kafka通过分区来实现数据冗余和伸缩性。分区可以分布在不同的服务器上，也就是说，一个主题可以横跨多个服务器，以此来提供比单个服务器更强大的性能。

人们把一个主题的数据看成一个流，不管它有多少个分区。

流是一组从生产者移动到消费者的数据。当我们讨论流式数据时，一般都是这样描述消息的。

Kafka stream、Apache samza 和strom 这些框架以实时的方式处理消息，也就是所谓的流式处理。

#### 1.2.4生产者和消费者
生产者在默认情况下把消息均衡地分布到主题的所有分区上，而并不关心特定消息会被写到哪个分区。

    在某些情况下，生产者会把消息直接写到指定的分区，这通常是通过消息键和分区器来实现的，分区器为键生成一个散列值，并将其映射到指定的分区上，这样可以保证包含同一个键的消息会被写到同一个分区上。
    生产者也可以使用自定义的分区器，根据不同的业务规则将消息映射到分区。
    消费者通过检查消息的偏移量来区分已经读取过的消息。偏移量是另一种元数据，它是一个不断递增的整数值，在创建消息时，Kafka会把它添加到消息里。
    在给定的分区里，每个消息的偏移量都是唯一的。

消费者把每个分区最后读取的消息偏移量保存在zookeeper或Kafka上，如果消费者关闭或重启，它的读取状态不会丢失。

    消费者是消费者群组的一部分，也就是说，会有一个或多个消费者共同读取一个主题。
    群组保证每个分区只能被一个消费者使用。
    消费者与分区之间的映射通常被称为消费者对分区的所有权关系。
    通过这种方式，消费者可以消费包含大量消息的主题。而且，如果一个消费者失效，群组里的其他消费者可以接管失效消费者的工作。

#### 1.2.5 broker和集群
一个独立的Kafka服务器被称为broker。

    broker接收来自生产者的消息，为消息设置偏移量，并提交消息到磁盘保存。
    broker为消费者提供服务，对读取分区的请求做出响应，返回已经提交到磁盘上的消息。

根据特定的硬件及其性能特征，单个broker可以轻松处理数千个分区以及每秒百万级的消息量。

    broker是集群的组成部分。每个集群都有一个broker同时充当了集群控制器的角色（自动从集群的活跃成员中选举出来）。
    控制器负责管理工作，包括将分区分配给broker和监控broker。
    在集群中，一个分区从属于一个broker，该broker被称为分区的首领。

一个分区可以分配给多个broker，这个时候会发生分区复制。这种复制机制为分区提供了消息冗余，如果有一个broker失效，其他broker可以接管领导权。不过，相关的消费者和生产者都要重新连接到新的首领。

##### 保留消息
Kafka broker默认的消息保留策略是：要么保留一段时间，要么保留到消息达到一定大小的字节数。

当消息数量达到这些上限时，旧消息就会过期并被删除。

主题可以配置自己的保留策略，可以将消息保留到不再使用它们为止。例如：用于跟踪用户活动的数据可能需要保留几天，而应用程序的度量指标可能只需要 保留几个小时。可以通过配置把主题当作紧凑型日志，只有最后一个带有特定键的消息会被保留下来。

#### 1.2.6 多集群
随着Kafka部署数量的增加，基于以下几点原因，最好使用多个集群。

    （1）数据类型分离
    （2）安全需求隔离
    （3）多数据中心（灾难恢复）
如果使用多个数据中心，就需要在它们之间复制消息。这样，在线应用程序才可以访问到多个站点的用户活动信息。

Kafka的消息复制机制只能在单个集群里进行。

Kafka提供了一个叫做 MirrorMaker的工具，可以用来实现集群间的消息复制。

Marror Maker的核心组件包含了一个生产者和一个消费者，两者之间通过一个队列相连。


### 1.3为什么选择Kafka
#### 1. 多个生产者
#### 2.多个消费者
Kafka支持多个消费者从一个单独的消息流上读取数据，而且消费者之间互不影响。

另外，多个消费者可以组成一个群组，它们共享一个消息流，并保证整个群组对每个给定的消息只处理一次。

#### 3.基于磁盘的数据存储
每个主题可以设置单独的保留规则，各个主题可以保留不同数量的消息。

消费者可能会因为处理速度慢或突发的流量高峰导致无法及时读取消息，而持久化数据可以保证数据不会丢失。

#### 4.伸缩性
单个、3个broker、上百。。。。在线扩展

要提高集群的容错能力，需要配置较高的复制系数。

#### 5.高性能
亚秒级的消息延迟


### 1.5 数据生态系统

#### 数据处理
    定义输入和应用程序，负责生成数据或者把数据引入系统。
    定义输出，可以是度量指标、报告或者其他类型的数据。
    创建一些循环，使用一些组件从系统读取数据，对读取的数据进行处理，然后把它们导到数据基础设施上，以备不时之需。
    数据类型可以多种多样，每一种数据类型可以有不同的内容、大小和用途。

Kafka为数据生态系统带来了循环系统，它在基础设施各个组件之间传递消息，为所有客户端提供一致的接口。

当与提供消息模式的系统集成时，生产者和消费者之间不再有紧密的耦合，也不需要在它们之间建立任何类型的直连。

#### 使用场景：
##### 1.活动跟踪
    跟踪用户的活动。
    静态信息：页面访问次数和点击量
    复杂操作：添加用户资料
这些消息被发布到一个或多个主题，由后端应用程序负责读取。这样我们就可以生成报告，为机器学习系统提供数据，更新搜索结果，或者实现其他更多的功能。

##### 2.传递消息
公共组件：格式化消息、将多个消费放在同一个通知里发送、根据用户配置的首选项来发送数据。

好处：不需要在多个应用程序上开发重复的功能，而且可以在公共组件上做一些有趣的转换，比如把多个消息聚合成一个单独的通知，而这些工作是无法在其他地方完成的。

##### 3.度量指标和日志记录
Kafka可以用于收集应用程序和系统度量指标以及日志。

离线系统较长时间的数据分析，比如年度增长走势预测

##### 4.提交日志
把数据库的更新发布到Kafka上，应用程序通过监控事件流来接收数据库的实时更新。

用于把数据库的更新复制到远程系统上，或者合并多个应用程序的更新到一个单独的数据库视图上。重放日志来恢复系统状态。另外紧凑型日志主题只为每个键保留一个变更数据，所以可以长时间使用，不需要担心消息过期问题。

##### 5.流处理
Kafka操作实时数据流。Hadoop处理更长时间片段的数据，Hadoop会对数据进行批处理。

通过使用流式处理框架，用户可以编写小型应用程序来操作Kafka消息，比如：计算度量指标，为其他应用程序有效的处理消息分区，或者对来自多个数据源的消息进行转换。


